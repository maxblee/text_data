{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar and Different Documents\n",
    "\n",
    "One of the core tasks of text-based data analysis (or of natural language processing in general) is figuring out which documents are similar to one another and which ones are different. Let's say you have a ton of financial documents &mdash; far too many to read manually &mdash; a few of which you know are interesting. You want to find more documents that look like the interesting documents you have.\n",
    "\n",
    "Even if you haven't done this yourself, you've probably seen something like this. Text-based recommendation systems, like the ones at *The New York Times* that suggest a story for you to read after your current article, almost certainly incorporate document similarity as a key feature.\n",
    "\n",
    "In addition to being a feature, and sometimes a standalone task, in natural language processing, document similarity closely resembles a number of other tasks. When you're clustering a set of documents into machine-learned topics, you're finding groups of documents that are similar to one another. And if you're classifying a set of documents, you're kind of creating a binary cluster of documents that resemble each other, at least in one specific way.\n",
    "\n",
    "So what does this have to do with `text_data`, which is a data exploration library? Well, whether you're trying to create a model for a web application, for some data analysis, or to strengthen a journalistic story, you'll want some intuition about *why* your computer thinks some documents are similar to one another and why they're different. You want to make sure that your model is picking up on what you want it to pick up on and is not picking up on things you don't want it to pick up on. It's very easy to create biased models; data exploration is a key way to limit those biases and improve the performance of your models.\n",
    "\n",
    "There's only one function in `text_data` specifically geared toward helping you explore similar documents. `distance_heatmap` allows you to graphically render similarity scores between two corpora (or between one corpus and itself). This requires a \"distance matrix,\" or a matrix of pairwise distances between all of the documents in one corpus to all of the documents in another. There are a ton of different ways to create these matrices, but the basic strategy you'll take will typically go as follows:\n",
    "\n",
    "1. You'll tokenize all of the documents, or in other words convert each document into a list of strings\n",
    "2. You'll build some sort of model that converts the tokenized documents into a vector or matrix of numbers (called an encoding).\n",
    "3. You'll create an pairwise matrix between the two corpuses, setting the values of the cells as the similarity or distance of the two documents at their respective locations. (Typically, you'll use cosine similarity.)\n",
    "\n",
    "For this particular, notebook, I'll be using `Doc2Vec`. Another good option is the Universal Sentence Encoder. But feel free to get creative with this. As long as you end up with a matrix of numbers when all is said and done, you should be good.\n",
    "\n",
    "## Set Up\n",
    "\n",
    "To start, you'll need to install some packages. This notebook uses `altair_saver` to save data visualizations as images, `pandas` to deal with data selection and manipulation, and `gensim` to create our model. You can install them in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install altair_saver pandas gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Next, you'll want to load in the data. For all of the examples, I'm using the [Kaggle State of the Union Corpus](https://www.kaggle.com/rtatman/state-of-the-union-corpus-1989-2017) for all of the notebooks in this examples directory. This dataset contains the text of all of the State of the Union Addresses between 1790 and 2018, except for 1933."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>year</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bush</td>\n",
       "      <td>2001</td>\n",
       "      <td>To the Congress of the United States:\\n\\nMr. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monroe</td>\n",
       "      <td>1822</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1794</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleveland</td>\n",
       "      <td>1895</td>\n",
       "      <td>To the Congress of the United States:\\n\\nThe p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bush</td>\n",
       "      <td>2008</td>\n",
       "      <td>Madam Speaker, Vice President Cheney, Members ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    president  year                                             speech\n",
       "0        Bush  2001  To the Congress of the United States:\\n\\nMr. S...\n",
       "1      Monroe  1822  Fellow-Citizens of the Senate and House of Rep...\n",
       "2  Washington  1794  Fellow-Citizens of the Senate and House of Rep...\n",
       "3   Cleveland  1895  To the Congress of the United States:\\n\\nThe p...\n",
       "4        Bush  2008  Madam Speaker, Vice President Cheney, Members ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import text_data\n",
    "from utilities import load_sotu_data, tokenizer\n",
    "\n",
    "sotu_speeches = pd.DataFrame(load_sotu_data())\n",
    "sotu_speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pairwise distance matrix\n",
    "\n",
    "Next, I'm going to create a document embedding model and use it to create a matrix of similar (or dissimilar) documents. I'm using `Doc2Vec` for this. `Doc2Vec` is a slight extension of `word2vec` that creates vector representations of both documents and words. We'll use it to get a sense of which documents are similar to other documents. However, depending on your context, other methods might work. Just be careful about the length of the documents you're looking at: some methods perform very well for small documents (like tweets) but perform poorly for longer documents (like the State of the Union Addresses we're looking at).\n",
    "\n",
    "To start, I'll create a model and use that to create a pairwise matrix of document similarities across the entire State of the Union corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_corpus = text_data.Corpus(list(sotu_speeches.speech), tokenizer)\n",
    "documents = [\n",
    "    doc2vec.TaggedDocument(doc, [i])\n",
    "    for i, doc in enumerate(sotu_corpus.tokenized_documents)\n",
    "]\n",
    "# hyperparameters; feel free to experiment with these; this is just intended as a fast and dirty\n",
    "# *ok* representation of the documents\n",
    "doc2vec_params = {\n",
    "    \"alpha\": 0.025,\n",
    "    \"epochs\": 10,\n",
    "    \"min_alpha\": 0.0001,\n",
    "    # this culls rare words from the trained model,\n",
    "    # which lowers the memory cost of the model and *should* improve performance\n",
    "    \"min_count\": 5,\n",
    "    # this just speeds up performance on multi-core machines\n",
    "    \"workers\": max(1, multiprocessing.cpu_count() // 2)\n",
    "}\n",
    "model = doc2vec.Doc2Vec(documents, **doc2vec_params)\n",
    "distance_matrix = np.array(list(map(model.docvecs.distances, range(len(documents)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11 s ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sotu_corpus.add_ngram_index(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 47.64 MiB, increment: 0.22 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import text_data\n",
    "from utilities import load_sotu_data, tokenizer\n",
    "\n",
    "sotu_speeches = pd.DataFrame(load_sotu_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text_data.Corpus(list(sotu_speeches.speech), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t0.09286477736412128\n",
      "of\t0.05791770487410226\n",
      "and\t0.03570726898185784\n",
      "to\t0.03567010709502323\n",
      "in\t0.02236732086865637\n",
      "a\t0.016178321672267786\n",
      "that\t0.012621529525096691\n",
      "for\t0.010882052536303649\n",
      "be\t0.010669679065834813\n",
      "our\t0.009902744892902372\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['the'], dtype='<U10'), array([3.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = text_data.Corpus([\"The cat is near the birds\", \"The birds are distressed\"])\n",
    "corpus.get_top_words(corpus.word_count_vector(), top_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['the', 'birds'], dtype='<U10'), array([3., 2.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_top_words(corpus.word_count_vector(), top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_corpus = text_data.Corpus(list(sotu_speeches.speech[:5]), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['the', 'of'], dtype='<U16'), array([2379., 1540.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq = sotu_corpus.index.tf_matrix()\n",
    "sotu_corpus.get_top_words(sotu_corpus.index.word_count_vector(), top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_corpus.index.document_count_vector().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23442"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_corpus.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250650204"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l, r = numpy.shape(term_doc)\n",
    "l * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_corpus.ngram_indexes[3].index.term_doc_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 117, 168, 169, 173, 197, 213]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_corpus.ngram_indexes[3].index.get_docs_with_word(\"corner of the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53 s ± 4.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [tokenizer(doc) for doc in list(sotu_speeches.speech)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.04 s ± 55.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from text_data.core import tokenize_many\n",
    "\n",
    "%timeit tokenize_many(list(sotu_speeches.speech), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Similarities\n",
    "\n",
    "Next, let's see what State of the Union Addresses are similar to each other. To do this, we'll create a heat map using `text_data` that takes our similarity matrix and graphically renders it so similar addresses will have a lighter color, while considerably different addresses will have a darker color.\n",
    "\n",
    "There are a few things to note here. First of all, you should see a very light diagonal straight down the middle. These are comparing the similarity of one document to itself. Naturally, our model knows that the two identical documents are pretty much the same.\n",
    "\n",
    "But you should also see a structure to this visualization. There are entire blocks of light colors and blocks of dark colors. If you look carefully at the indices, you'll notice that they appear in alphabetical order. What's happening is something that probably makes intuitive sense to you: the State of the Union Addresses that individual presidents give are pretty similar *to one another*.\n",
    "\n",
    "You can also see some patterns or outliers in the data that are interesting (but make sense). President George W. Bush's addresses look fairly similar to a lot of addresses from presidents that are fairly contemporary to him &mdash; presidents like Obama, Clinton, and Reagan. Finally, you can see that George Washington's inaugural address really looks unlike any of the other addresses (with the exception of itself); the heatmap is basically a dark blue line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_indices = sotu_speeches.president + \", \" + sotu_speeches.year\n",
    "chart = text_data.multi_corpus.distance_heatmap(\n",
    "    distance_matrix,\n",
    "    speech_indices,\n",
    "    speech_indices,\n",
    "    \"SOTU Speech\",\n",
    "    \"SOTU Speech\"\n",
    ")\n",
    "chart.save(\"speech_similarity_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A heatmap showing which State of the Union Addresses are Similar to Each Other](speech_similarity_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the first heatmap compares the entire corpus to itself, there's no need to do this. Here, I'm going to compare the speeches of Clinton and Obama to those of George W. and George HW Bush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "roosevelts = sotu_speeches[sotu_speeches.president == \"Roosevelt\"]\n",
    "post_war = sotu_speeches[\n",
    "    (sotu_speeches.president == \"Truman\") | (sotu_speeches.president == \"Eisenhower\")\n",
    "]\n",
    "compare_matrix = distance_matrix[roosevelts.index][:,post_war.index]\n",
    "heatmap = text_data.multi_corpus.distance_heatmap(\n",
    "    compare_matrix,\n",
    "    speech_indices.loc[roosevelts.index],\n",
    "    speech_indices.loc[post_war.index],\n",
    "    \"Roosevelt Speeches\",\n",
    "    \"Clinton and Obama Speeches\"\n",
    ")\n",
    "heatmap.save(\"recent_sotu_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A heatmap of the two Roosevelts, Truman and Eisenhower](recent_sotu_heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there's a pattern here, this one temporal. As you'll see, the first Roosevelt's speeches have a lot less in common to Eisenhower's and Truman's speeches. That makes sense, considering that the second Roosevelt served during the same general timeframe as the other two (even if the Second World War complicates things.\n",
    "\n",
    "## Final Exploration\n",
    "\n",
    "There's a lot more you can do with `text_data` to explore these similarities. I'm going to explore one way and hint at how you might extend that. In particular, I'm going to find the documents that are most similar to FDR's \"Four Freedoms\" speech, the 1941 State of the Union Address where FDR  argued that the United States needed to serve as \"an arsenal\" for its allies and spoke of democratic principles as they were most under attack.\n",
    "\n",
    "I'm going to start doing this by getting the 25 speeches that `Doc2Vec` thinks are most similar to this one, and I'm going to plot the top 15 results in `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>year</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Truman</td>\n",
       "      <td>1951</td>\n",
       "      <td>Mr. President, Mr. Speaker, Members of the Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Roosevelt</td>\n",
       "      <td>1940</td>\n",
       "      <td>Mr. Vice President, Mr. Speaker, Members of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Roosevelt</td>\n",
       "      <td>1943</td>\n",
       "      <td>Mr. Vice President, Mr. Speaker, Members of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Roosevelt</td>\n",
       "      <td>1944</td>\n",
       "      <td>To the Congress:\\n\\nThis Nation in the past tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Roosevelt</td>\n",
       "      <td>1942</td>\n",
       "      <td>In fulfilling my duty to report upon the State...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Roosevelt</td>\n",
       "      <td>1939</td>\n",
       "      <td>Mr. Vice President, Mr. Speaker, Members of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Truman</td>\n",
       "      <td>1953</td>\n",
       "      <td>To the Congress of the United States:\\n\\nI hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>1963</td>\n",
       "      <td>Mr. Vice President, Mr. Speaker, Members of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Roosevelt</td>\n",
       "      <td>1945</td>\n",
       "      <td>To the Congress:\\n\\nIn considering the State o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Truman</td>\n",
       "      <td>1952</td>\n",
       "      <td>Mr. President, Mr. Speaker, Members of the Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>1958</td>\n",
       "      <td>Mr. President, Mr. Speaker, Members of the 85t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>1967</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, distinguished...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Nixon</td>\n",
       "      <td>1970</td>\n",
       "      <td>Mr. Speaker, Mr. President, my colleagues in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>1960</td>\n",
       "      <td>Mr. President, Mr. Speaker, Members of the 86t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>1966</td>\n",
       "      <td>Mr. Speaker, Mr. President, Members of the Hou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      president  year                                             speech\n",
       "61       Truman  1951  Mr. President, Mr. Speaker, Members of the Con...\n",
       "161   Roosevelt  1940  Mr. Vice President, Mr. Speaker, Members of th...\n",
       "26    Roosevelt  1943  Mr. Vice President, Mr. Speaker, Members of th...\n",
       "183   Roosevelt  1944  To the Congress:\\n\\nThis Nation in the past tw...\n",
       "21    Roosevelt  1942  In fulfilling my duty to report upon the State...\n",
       "45    Roosevelt  1939  Mr. Vice President, Mr. Speaker, Members of th...\n",
       "221      Truman  1953  To the Congress of the United States:\\n\\nI hav...\n",
       "32      Kennedy  1963  Mr. Vice President, Mr. Speaker, Members of th...\n",
       "157   Roosevelt  1945  To the Congress:\\n\\nIn considering the State o...\n",
       "56       Truman  1952  Mr. President, Mr. Speaker, Members of the Con...\n",
       "96   Eisenhower  1958  Mr. President, Mr. Speaker, Members of the 85t...\n",
       "117     Johnson  1967  Mr. Speaker, Mr. Vice President, distinguished...\n",
       "203       Nixon  1970  Mr. Speaker, Mr. President, my colleagues in t...\n",
       "107  Eisenhower  1960  Mr. President, Mr. Speaker, Members of the 86t...\n",
       "204     Johnson  1966  Mr. Speaker, Mr. President, Members of the Hou..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_freedoms = sotu_speeches[sotu_speeches.year == \"1941\"].index[0]\n",
    "freedom_idxs, _ranks = zip(*model.docvecs.most_similar(four_freedoms, topn=25))\n",
    "near_four_freedoms = sotu_speeches.loc[list((*freedom_idxs, four_freedoms))]\n",
    "near_four_freedoms.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictably, there are a lot of speeches from Roosevelt in that list (and interestingly, they're speeches he gave during the war). You can also see a fair amount of temporal clustering. But now I want to hint at how this kind of thing can be extended.\n",
    "\n",
    "I'm going to find the words that most distinguish the documents that are similar to Roosevelt's Four Freedoms speech from a background corpus of all of the SOTU speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Low Scores</b></p><table><thead><tr><th>Order</th><th>Word</th><th>Score</th></tr></thead><tbody><tr><td>1.</td><td>officers</td><td>-3.0808350286828308</td></tr><tr><td>2.</td><td>objects</td><td>-3.117887798901709</td></tr><tr><td>3.</td><td>tribes</td><td>-3.1245334627833383</td></tr><tr><td>4.</td><td>notes</td><td>-3.163506158320878</td></tr><tr><td>5.</td><td>ports</td><td>-3.185558219649298</td></tr><tr><td>6.</td><td>silver</td><td>-3.2192582319201986</td></tr><tr><td>7.</td><td>claims</td><td>-3.2314176003162762</td></tr><tr><td>8.</td><td>intercourse</td><td>-3.400612267993276</td></tr><tr><td>9.</td><td>mexico</td><td>-3.4735328097059446</td></tr><tr><td>10.</td><td>spain</td><td>-3.600418221679119</td></tr></tbody></table><p><b>High Scores</b></p><table><thead><tr><th>Order</th><th>Word</th><th>Score</th></tr></thead><tbody><tr><td>1.</td><td>kremlin</td><td>2.5861379733056165</td></tr><tr><td>2.</td><td>hitler</td><td>2.5861379733056165</td></tr><tr><td>3.</td><td>nazis</td><td>2.5861242095684798</td></tr><tr><td>4.</td><td>appeasement</td><td>2.5861035643758257</td></tr><tr><td>5.</td><td>hemispheric</td><td>2.586096682755084</td></tr><tr><td>6.</td><td>numerically</td><td>2.5860898011894147</td></tr><tr><td>7.</td><td>hats</td><td>2.5860898011894147</td></tr><tr><td>8.</td><td>amphibious</td><td>2.5860898011894147</td></tr><tr><td>9.</td><td>versus</td><td>2.5860829196788124</td></tr><tr><td>10.</td><td>stalinist</td><td>2.5860829196788124</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_freedoms_corpus = text_data.Corpus(near_four_freedoms.speech.to_list(), tokenizer)\n",
    "\n",
    "text_data.multi_corpus.display_top_differences(\n",
    "    four_freedoms_corpus,\n",
    "    sotu_corpus,\n",
    "    text_data.multi_corpus.log_odds_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictably, words related to the Nazis and to wars are *highly associated* with the Four Freedoms speech and similar speeches, while old-timey words about silver or Mexico are much less common.\n",
    "\n",
    "Finally, we can look to see examples of certain words appearing. (We could do the same with entire documents, but the State of the Union Addresses are probably too long for this to be all that useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Showing Result 0 (Document ID 25)</b></p><p style='white-space=pre-wrap;'><b>&hellip;</b>fthearted; but we\n",
       "cannot afford to be soft-headed.\n",
       "\n",
       "We must always be wary of those who with sounding brass and a tinkling\n",
       "cymbal preach the \"ism\" of <b>appeasement</b>.\n",
       "\n",
       "We must especially beware of that small group of selfish men who would clip\n",
       "the wings of the American eagle in order to feather their own nests.\n",
       "\n",
       "I<b>&hellip;</b></p><p><b>Showing Result 1 (Document ID 18)</b></p><p style='white-space=pre-wrap;'><b>&hellip;</b>e community\n",
       "of nations holds dear. The world has said this aggression would not stand,\n",
       "and it will not stand.\n",
       "\n",
       "Together, we have resisted the trap of <b>appeasement</b>, cynicism and isolation\n",
       "that gives temptation to tyrants. The world has answered Saddam's invasion\n",
       "with 12 United Nations resolutions, starting with <b>&hellip;</b></p><p><b>Showing Result 2 (Document ID 0)</b></p><p style='white-space=pre-wrap;'><b>&hellip;</b>aid\n",
       "down in its charter.\n",
       "\n",
       "We are willing, as we have always been, to negotiate honorable settlements\n",
       "with the Soviet Union. But we will not engage in <b>appeasement</b>.\n",
       "\n",
       "The Soviet rulers have made it clear that we must have strength as well as\n",
       "right on our side. If we build our strength--and we are building it--the<b>&hellip;</b></p><p><b>Showing Result 3 (Document ID 9)</b></p><p style='white-space=pre-wrap;'><b>&hellip;</b> road to peace. We have increased the power and unity of the free\n",
       "world. And while we were doing this, we have avoided world war on the one\n",
       "hand, and <b>appeasement</b> on the other. This is a hard road to follow, but the\n",
       "events of the last year show that it is the right road to peace.\n",
       "\n",
       "We cannot expect to complete t<b>&hellip;</b></p><p><b>Showing Result 4 (Document ID 7)</b></p><p style='white-space=pre-wrap;'><b>&hellip;</b>hat, in the end, it is the only way\n",
       "to assure the security of all without impairing the interests of any. Nor\n",
       "do we mistake honorable negotiation for <b>appeasement</b>. While we shall never\n",
       "weary in the defense of freedom, neither shall we ever abandon the pursuit\n",
       "of peace.\n",
       "\n",
       "In this quest, the United Nations require<b>&hellip;</b></p><p><b>Showing Result 5 (Document ID 16)</b></p><p style='white-space=pre-wrap;'><b>&hellip;</b>at our presence in Berlin, our\n",
       "free access thereto, and the freedom of two million West Berliners would\n",
       "not be surrendered either to force or through <b>appeasement</b>--and to maintain\n",
       "those rights and obligations, we are prepared to talk, when appropriate,\n",
       "and to fight, if necessary. Every member of NATO stands wit<b>&hellip;</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_freedoms_corpus.display_search_results(\"appeasement\", window_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There's a lot more that you could do to explore this data. But hopefully this provides a decent overview of how you can use the heatmap function in `text_data` to figure out what documents are similar to each other and to visually identify patterns. And hopefully, you can imagine ways in which you could take information you have about document similarity &mdash; either from this library or from another &mdash; and search through those similar (or different documents) to try to identify meaningful patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_data",
   "language": "python",
   "name": "text_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
